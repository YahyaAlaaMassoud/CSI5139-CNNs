{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all the packages that are going to be used thoughout the assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "# ! pip install numpy\n",
    "# ! pip install matplotlib\n",
    "# ! pip install scikit-learn\n",
    "# ! pip install scikit-image\n",
    "\n",
    "%autosave 60\n",
    "%matplotlib inline\n",
    "import os\n",
    "import skimage\n",
    "from skimage.transform import rescale, resize\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import ntpath\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"1_1\">1.1 Getting Started</a>\n",
    "***\n",
    "I've used my implementation of the data reading part from Assignment 1 of this course, as I mentioned before it is generic and could be applied to any other data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subdirectories(dir_name):\n",
    "    '''\n",
    "        This functions returns a list of all subdirectories' paths under a specific directory name.\n",
    "        \n",
    "        params:\n",
    "            dir_name: str\n",
    "        returns:\n",
    "            subdirectories_list: List[str]\n",
    "    '''\n",
    "    if (os.path.isdir(dir_name)):\n",
    "        return [os.path.join(dir_name, subdir_name) for subdir_name in os.listdir(dir_name) if os.path.isdir(os.path.join(dir_name, subdir_name))]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def get_files_within_dir(dir_name, ext):\n",
    "    '''\n",
    "        This function returns all the files' paths withing a specific directory with the same extension as [ext]\n",
    "        \n",
    "        params:\n",
    "            dir_name: str\n",
    "            ext: str\n",
    "        returns:\n",
    "            all_files_found_list: List[str]\n",
    "    '''\n",
    "    if (os.path.isdir(dir_name)):\n",
    "        return [os.path.join(dir_name, file_name) for file_name in os.listdir(dir_name) if file_name.endswith(ext)]\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "def read_images_from_files(filenames, rescale_factor, rgb=True, flatten=False):\n",
    "    '''\n",
    "        This function takes a list of images' filenames, reads all the images, and return them based on some parameters given,\n",
    "        to the function as the rescaling factor, either RGB or grayscale, and either the image will be flattened or not\n",
    "        \n",
    "        params: \n",
    "            filenames: List[str]\n",
    "            rescale_factor: float\n",
    "            rgb: bool (optional) default=True\n",
    "            flatten: bool (optional) default=False\n",
    "        return:\n",
    "            List[Tuple[np.ndarray, Tuple[int, int], str]] -> List[(image, original_size, filename)]\n",
    "    '''\n",
    "    def read_image_from_file(filename):\n",
    "        '''\n",
    "            This function takes an image's filename, read the image pixels in either RGB or grayscale format as specified,\n",
    "            resizes the image with the given rescaling factor, flattens the image so it can be one vector on numbers,\n",
    "            and returns the preprocessed image and its original shape\n",
    "            \n",
    "            params: \n",
    "                filename: str\n",
    "            returns:\n",
    "                img: np.ndarray\n",
    "                org_size: Tuple[int, int]\n",
    "        '''\n",
    "        if rgb:\n",
    "            flag = 1\n",
    "        else:\n",
    "            flag = 0\n",
    "        img = np.asarray(Image.open(filename))#cv2.imread(filename, flag)\n",
    "        org_size = img.shape\n",
    "        img = cv2.resize(img, (int(org_size[0] * rescale_factor), int(org_size[1] * rescale_factor)))\n",
    "        \n",
    "        if (flatten==True):\n",
    "            img = img.flatten()\n",
    "        return img, org_size\n",
    "        \n",
    "    images = []\n",
    "    for filename in filenames:\n",
    "        img, org_size = read_image_from_file(filename)\n",
    "        images.append((img, org_size, filename))\n",
    "    return images\n",
    "\n",
    "def get_all_images(dataset_dirname, rescale_factor, RGB=True, flatten=False):\n",
    "    '''\n",
    "        This function acts as the main API function, and is responsible for fetching and preprocessing all the images\n",
    "        within the dataset directory, it uses the already defined helper functions\n",
    "        \n",
    "        params:\n",
    "            dataset_dirname: str\n",
    "            rescale_factor: float\n",
    "            RGB: bool\n",
    "            flatten: bool\n",
    "        return:\n",
    "            all_images: List[Tuple[np.ndarray, Tuple[int, int], str]] -> List[(image, original_size, filename)]\n",
    "    '''\n",
    "    subdirectories = get_subdirectories(dataset_dirname)\n",
    "    all_images = []\n",
    "    for subdir in subdirectories:\n",
    "        images_filenames = get_files_within_dir(subdir, '.png')\n",
    "        subdir_images = read_images_from_files(filenames=images_filenames, rescale_factor=rescale_factor, rgb=RGB, flatten=flatten)\n",
    "        all_images += subdir_images\n",
    "    return all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Defining the global variables that I will be using to extract and pre-process the image data\n",
    "        - Dataset directory name (train/test)\n",
    "        - Rescaling factor as mentioned in the assignment PDF (1/4) -> ((256x256) -> (64x64))\n",
    "'''\n",
    "TRAIN_VAL_DIRNAME = \"sub1to10\"\n",
    "TEST_DIRNAME = \"sub31to40\"\n",
    "RESCALING_FACTOR = 1. / 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_data = get_all_images(TRAIN_VAL_DIRNAME, RESCALING_FACTOR, RGB=True, flatten=False)\n",
    "test_data = get_all_images(TEST_DIRNAME, RESCALING_FACTOR, RGB=True, flatten=False)\n",
    "print(\"Number of train/val images {0}\".format(len(train_val_data)))\n",
    "print(\"Number of test images {0}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"1_2\">1.2 Template Matching</a>\n",
    "***\n",
    "- #### [1.2.1 Construct arrays for all pairs of images](#1_2_1)\n",
    "- #### [1.2.2 Split the data into train/validation sets](#1_2_2)\n",
    "- #### [1.2.3 Defining the similarity function](#1_2_3)\n",
    "- #### [1.2.4 Choosing the right threshold](#1_2_4)\n",
    "- #### [1.2.5 Evaluating the classification threshold](#1_2_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"1_2_1\">1.2.1 Construct arrays for all pairs of images</a>\n",
    "***\n",
    "Since storing all pairs of images would be a huge load on the memory, so I just construct array of pairs of images using only their indices in the main images array (*train_val_data*) along with the label (whether two images are the same or not). <br>\n",
    "I also make sure that I have array for (same faces, different faces, all faces), to be able to split the data fairly into training and validation data, as there is already a hust data imbalance (10K same faces vs. 95K different faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_pairs = []\n",
    "train_val_pairs_same = []\n",
    "train_val_pairs_diff = []\n",
    "vis = {}\n",
    "\n",
    "for i in range(len(train_val_data)):\n",
    "    for j in range(len(train_val_data)):\n",
    "        if (i != j and (i, j) not in vis and (j, i) not in vis):\n",
    "            _, _, f1 = train_val_data[i]\n",
    "            _, _, f2 = train_val_data[j]\n",
    "\n",
    "            name1, name2 = f1.split('\\\\')[1][:-1], f2.split('\\\\')[1][:-1]\n",
    "\n",
    "            if (name1 == name2):\n",
    "                same = 1\n",
    "            else:\n",
    "                same = 0\n",
    "\n",
    "            label = tf.keras.utils.to_categorical(same, num_classes=2)\n",
    "            train_val_pairs.append((i, j, same))\n",
    "\n",
    "            if same == 1:\n",
    "                train_val_pairs_same.append((i, j, same))\n",
    "            else:\n",
    "                train_val_pairs_diff.append((i, j, same))\n",
    "            vis[(i, j)] = True\n",
    "\n",
    "test_pairs = []\n",
    "test_pairs_same = []\n",
    "test_pairs_diff = []\n",
    "vis = {}\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    for j in range(len(test_data)):\n",
    "        if (i != j and (i, j) not in vis and (j, i) not in vis):\n",
    "            _, _, f1 = test_data[i]\n",
    "            _, _, f2 = test_data[j]\n",
    "      \n",
    "            name1, name2 = f1.split('\\\\')[1][:-1], f2.split('\\\\')[1][:-1]\n",
    "      \n",
    "            if (name1 == name2):\n",
    "                same = 1\n",
    "            else:\n",
    "                same = 0\n",
    "\n",
    "            label = tf.keras.utils.to_categorical(same, num_classes=2)\n",
    "            test_pairs.append((i, j, same))\n",
    "\n",
    "            if same == 1:\n",
    "                test_pairs_same.append((i, j, same))\n",
    "            else:\n",
    "                test_pairs_diff.append((i, j, same))\n",
    "            vis[(i, j)] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"1_2_2\">1.2.2 Split the data into train/validation sets</a>\n",
    "***\n",
    "Here I shuffle the data, then take differnt train portions of both (same/different faces), and the same for the validation portions.<br>\n",
    "Then I construct a training set using a mix of same/difference faces, and a validation set using a mix of same/different faces. <br>\n",
    "Doing that insures that there will always be same/different faces in both training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train same 10350 - Test diff 95220\n",
      "Validation same 517 - Validation diff 4761\n",
      "train 100292 - val 5278\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# shuffle\n",
    "random.shuffle(train_val_pairs_same)\n",
    "random.shuffle(train_val_pairs_diff)\n",
    "# train_val_pairs_diff = random.sample(train_val_pairs_diff, int(len(train_val_pairs_same) * 5))\n",
    "# split\n",
    "print(\"Train same {0} - Test diff {1}\".format(len(train_val_pairs_same), len(train_val_pairs_diff)))\n",
    "\n",
    "val_same = 0.05\n",
    "val_diff = 0.05\n",
    "\n",
    "val_same_sz = int(val_same * len(train_val_pairs_same))\n",
    "val_diff_sz = int(val_diff * len(train_val_pairs_diff))\n",
    "\n",
    "print(\"Validation same {0} - Validation diff {1}\".format(val_same_sz, val_diff_sz))\n",
    "\n",
    "train_same = train_val_pairs_same[:-val_same_sz]\n",
    "train_diff = train_val_pairs_diff[:-val_diff_sz]\n",
    "val_same = train_val_pairs_same[-val_same_sz:]\n",
    "val_diff = train_val_pairs_diff[-val_diff_sz:]\n",
    "\n",
    "train_pairs = train_same + train_diff\n",
    "val_pairs = val_same + val_diff\n",
    "\n",
    "print(\"train {0} - val {1}\".format(len(train_pairs), len(val_pairs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"1_2_3\">1.2.3 Defining the similarity function</a>\n",
    "***\n",
    "I used *template_match* provided by **skimage**, which uses a fast normalized cross-correlation to find the similarity between a template and an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import match_template\n",
    "\n",
    "def compute_similarity(img1, img2):\n",
    "    score = match_template(img1, img2)\n",
    "    return np.round(np.squeeze(score), 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"1_2_4\">1.2.4 Choosing the right threshold</a>\n",
    "***\n",
    "In order to choose the threshold that could give us best classification accuracy:\n",
    "- First I computed the similarity scores for all the pairs in the validation set (same faces / different faces).\n",
    "- Since storing all these pairs of images could take time, I've implemented a generator for that, for an efficient use of memory.\n",
    "- I've saved my results in numpy arrays, to be able to load them whenever I want.\n",
    "- **The commented out code is the code that I already ran and saved its result, I am then loading the results from the numpy file as shown**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_generator(data_pairs, data):\n",
    "    for pair in data_pairs:\n",
    "        i, j, label = pair\n",
    "        left, right = data[i][0], data[j][0]\n",
    "        score = compute_similarity(left, right)\n",
    "        yield score\n",
    "# same_scores = []\n",
    "# for x in similarity_generator(val_same[:500], train_val_data):\n",
    "#     same_scores.append(x)\n",
    "# diff_scores = []\n",
    "# for x in similarity_generator(val_diff[:500], train_val_data):\n",
    "#     diff_scores.append(x)\n",
    "    \n",
    "# np.save('same_scores', same_scores)\n",
    "# np.save('diff_scores', diff_scores)\n",
    "same_scores = np.load('same_scores.npy')\n",
    "diff_scores = np.load('diff_scores.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason I calculated all the similarity scores of the pairs of faces, is to be able to see how my threshold of choise would affect the classification accuracy. <br>\n",
    "By observing these **box plots**:\n",
    "- The median of all the similirity scores for the same faces is above the 0.8 mark, and most of the scores are above the 0.75 mark.\n",
    "- The median of all the similarity scores for the different faces is below the 0.5 mark, and most of the scores are below the 0.6 mark.\n",
    "- In order to benifit the most from these observation, I chose to have a threshold in between the values 0.6 and 0.8, also to not be affected by the outliers.\n",
    "- **The chosen threshold would be 0.68 (the dotted line in blue).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "threshold = 0.68\n",
    "data = [same_scores, diff_scores]\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "fig.suptitle('Box plots for matching scores \\non same/different faces', fontsize=14, fontweight='bold')\n",
    "bp = ax.boxplot(data, notch=0, sym='+', vert=1, whis=1.5)\n",
    "ax.yaxis.grid(True, linestyle='-', which='major', color='lightgrey', alpha=0.5)\n",
    "ax.set_xticklabels(['same faces scores', 'different faces scores'], fontsize=12)\n",
    "plt.axhline(y=threshold, color='b', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"1_2_5\">1.2.5 Evaluating the classification threshold</a>\n",
    "***\n",
    "*The commented out code is a block of code that I already ran an stored its results*.<br><br>\n",
    "We can see that this threshold turns out to balance between the accuracy of the same faces and the different faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same faces accuracy: 0.8723 \n",
      "different faces accuracy: 0.9105 \n",
      "overall accuracy: 0.9068\n"
     ]
    }
   ],
   "source": [
    "# same = 0\n",
    "# diff = 0\n",
    "# for pair in val_same:\n",
    "#     i, j, label = pair\n",
    "#     left, right = train_val_data[i][0], train_val_data[j][0]\n",
    "#     score = compute_similarity(left, right)\n",
    "#     if (score > threshold):\n",
    "#         same += 1\n",
    "# for pair in val_diff:\n",
    "#     i, j, label = pair\n",
    "#     left, right = train_val_data[i][0], train_val_data[j][0]\n",
    "#     score = compute_similarity(left, right)\n",
    "#     if (score < threshold):\n",
    "#         diff += 1\n",
    "        \n",
    "# same_acc = np.round(same / len(val_same), 4)\n",
    "# diff_acc = np.round(diff / len(val_diff), 4)\n",
    "# overall_acc = np.round((same + diff) / (len(val_same) + len(val_diff)), 4)\n",
    "\n",
    "# np.save('threshold_68_acc', np.array([same_acc, diff_acc, overall_acc]))\n",
    "accuracies = np.load('threshold_68_acc.npy')\n",
    "print(\"same faces accuracy: {0} \\ndifferent faces accuracy: {1} \\noverall accuracy: {2}\".format(accuracies[0], accuracies[1], accuracies[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"1_3\">1.3 Perceptron</a>\n",
    "***\n",
    "- #### [1.3.1 Defining the MLP architecture](#1_3_1)\n",
    "- #### [1.3.2 Split the data into train/validation sets](#1_3_2)\n",
    "- #### [1.3.3 Defining the similarity function](#1_3_3)\n",
    "- #### [1.3.4 Choosing the right threshold](#1_3_4)\n",
    "- #### [1.3.5 Evaluating the classification threshold](#1_3_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"1_3_1\">1.3.1 Defining the MLP architecture</a>\n",
    "***\n",
    "The inpur representation to the MLP is just a pair of images concatenated. The output contains only 2 units (softmax classification). <br>\n",
    "\n",
    "*The commented out code of the MLP is a code that I ran on Google Colab GPU, and saved the results to be shown here later*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define two sets of inputs\n",
    "# inputLeft = tf.keras.layers.Input(shape=(64, 64, 3))\n",
    "# flatInputLeft = tf.keras.layers.Flatten()(inputLeft)\n",
    "# inputRight = tf.keras.layers.Input(shape=(64, 64, 3))\n",
    "# flatInputRight = tf.keras.layers.Flatten()(inputRight)\n",
    " \n",
    "# # the first branch operates on the first input\n",
    "# x = tf.keras.layers.Dense(256, activation=\"linear\")(flatInputLeft)\n",
    "# x = tf.keras.layers.BatchNormalization()(x)\n",
    "# x = tf.keras.layers.ReLU()(x)\n",
    "# x = tf.keras.layers.Dropout(0.2)(x)\n",
    "# x = tf.keras.layers.Dense(128, activation=\"linear\")(x)\n",
    "# x = tf.keras.layers.BatchNormalization()(x)\n",
    "# x = tf.keras.layers.ReLU()(x)\n",
    "# x = tf.keras.layers.Dropout(0.2)(x)\n",
    "# x = tf.keras.models.Model(inputs=inputLeft, outputs=x)\n",
    " \n",
    "# # the first branch operates on the first input\n",
    "# y = tf.keras.layers.Dense(256, activation=\"linear\")(flatInputRight)\n",
    "# y = tf.keras.layers.BatchNormalization()(y)\n",
    "# y = tf.keras.layers.ReLU()(y)\n",
    "# y = tf.keras.layers.Dropout(0.2)(y)\n",
    "# y = tf.keras.layers.Dense(128, activation=\"linear\")(y)\n",
    "# y = tf.keras.layers.BatchNormalization()(y)\n",
    "# y = tf.keras.layers.ReLU()(y)\n",
    "# y = tf.keras.layers.Dropout(0.2)(y)\n",
    "# y = tf.keras.models.Model(inputs=inputRight, outputs=y)\n",
    " \n",
    "# # combine the output of the two branches\n",
    "# combined = tf.keras.layers.concatenate([x.output, y.output])\n",
    " \n",
    "# # apply a FC layer and then a regression prediction on the combined outputs\n",
    "# z = tf.keras.layers.Dense(64, activation=\"linear\")(combined)\n",
    "# z = tf.keras.layers.BatchNormalization()(z)\n",
    "# z = tf.keras.layers.ReLU()(z)\n",
    "# z = tf.keras.layers.Dropout(0.1)(z)\n",
    "# z = tf.keras.layers.Dense(32, activation=\"linear\")(z)\n",
    "# z = tf.keras.layers.BatchNormalization()(z)\n",
    "# z = tf.keras.layers.ReLU()(z)\n",
    "# z = tf.keras.layers.Dropout(0.1)(z)\n",
    "# z = tf.keras.layers.Dense(16, activation=\"linear\")(z)\n",
    "# z = tf.keras.layers.BatchNormalization()(z)\n",
    "# z = tf.keras.layers.ReLU()(z)\n",
    "# z = tf.keras.layers.Dense(2, activation=\"softmax\")(z)\n",
    " \n",
    "# # our model will accept the inputs of the two branches and then output a single value\n",
    "# image_mlp = tf.keras.models.Model(inputs=[x.input, y.input], outputs=z)\n",
    "\n",
    "# adam = tf.keras.optimizers.Adam(learning_rate=1e-3,  decay=1e-2, amsgrad=True)\n",
    "# image_mlp.compile(optimizer=adam,\n",
    "#                   loss='categorical_crossentropy',\n",
    "#                   metrics=['accuracy'])\n",
    "\n",
    "# image_mlp.fit_generator(generator=train_generator, \n",
    "#                         validation_data=val_generator, \n",
    "#                         use_multiprocessing=True, \n",
    "#                         workers=6, \n",
    "#                         epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "  # reference -> https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly \n",
    "    def __init__(self, list_IDs, labels, batch_size=64, dim=(64,64), n_channels=3,\n",
    "                       n_classes=2, shuffle=True, data=None):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.data = data\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __normalize(self, x):\n",
    "        x_min = np.amin(x)\n",
    "        x_max = np.amax(x)\n",
    "        return (x - x_min) / (x_max - x_min)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X_left = []\n",
    "        X_right = []\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        # Generate data\n",
    "        for i, j, label in list_IDs_temp:\n",
    "            # Store sample\n",
    "\n",
    "            x_left = self.data[i][0]#self.__normalize(self.data[i][0])\n",
    "            x_right = self.data[j][0]#self.__normalize(self.data[j][0])\n",
    "            # print(x_left.shape, x_right.shape, label)\n",
    "            x_left = skimage.color.rgb2gray(x_left)\n",
    "            x_right = skimage.color.rgb2gray(x_right)\n",
    "            X_left.append(x_left)\n",
    "            X_right.append(x_right)\n",
    "\n",
    "            X.append(np.concatenate([x_left, x_right], axis=1))\n",
    "\n",
    "            # Store class\n",
    "            if self.n_classes > 1:\n",
    "                label = tf.keras.utils.to_categorical(label, num_classes=self.n_classes)\n",
    "            y.append(label)\n",
    "\n",
    "        X_left, X_right, X, y = np.array(X_left), np.array(X_right), np.array(X), np.array(y)\n",
    "        return X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_mlp_path = 'mlp/image_concat_2'\n",
    "\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "json_file = open(image_mlp_path + '.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "image_mlp = model_from_json(loaded_model_json)\n",
    "plot_model(image_mlp, to_file='image_mlp.png', show_shapes=True, show_layer_names=False)\n",
    "\n",
    "image_mlp.load_weights(image_mlp_path + '.h5')\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=1e-3,  decay=1e-2, amsgrad=True)\n",
    "image_mlp.compile(optimizer=adam,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator     = DataGenerator(train_pairs,     labels=[], data=train_val_data, batch_size=32,  n_classes=2)\n",
    "val_generator       = DataGenerator(val_pairs,       labels=[], data=train_val_data, batch_size=32,  n_classes=2)\n",
    "test_generator      = DataGenerator(test_pairs,      labels=[], data=test_data,      batch_size=128, n_classes=2)\n",
    "test_generator_same = DataGenerator(test_pairs_same, labels=[], data=test_data,      batch_size=128, n_classes=2)\n",
    "test_generator_diff = DataGenerator(test_pairs_diff, labels=[], data=test_data,      batch_size=128, n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.34371803702769527 - val accuracy: 0.9197789430618286\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = image_mlp.evaluate_generator(generator=val_generator, verbose=0, max_queue_size=5)\n",
    "print(\"val loss: {0} - val accuracy: {1}\".format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_loss, same_acc = image_mlp.evaluate_generator(generator=test_generator_same, verbose=0, max_queue_size=5)\n",
    "diff_loss, diff_acc = image_mlp.evaluate_generator(generator=test_generator_diff, verbose=0, max_queue_size=5)\n",
    "image_mlp_results = [same_loss, same_acc, diff_loss, diff_acc]\n",
    "np.save('image_mlp_results', image_mlp_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same loss: 0.08933231302283028 - same accuracy: 0.9691051244735718\n",
      "different loss: 1.0048775481661474 - different accuracy: 0.5789708495140076\n"
     ]
    }
   ],
   "source": [
    "same_loss, same_acc, diff_loss, diff_acc = np.load('image_mlp_results.npy')\n",
    "print(\"same loss: {0} - same accuracy: {1}\\ndifferent loss: {2} - different accuracy: {3}\".format(same_loss, same_acc, diff_loss, diff_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-166-72b883935700>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpair\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0msame\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-119-5e7354113751>\u001b[0m in \u001b[0;36mcompute_similarity\u001b[1;34m(img1, img2)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatch_template\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\ottawa\\lib\\site-packages\\skimage\\feature\\template.py\u001b[0m in \u001b[0;36mmatch_template\u001b[1;34m(image, template, pad_input, mode, constant_values)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         xcorr = fftconvolve(image, template[::-1, ::-1, ::-1],\n\u001b[1;32m--> 150\u001b[1;33m                             mode=\"valid\")[1:-1, 1:-1, 1:-1]\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[0mnumerator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxcorr\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mimage_window_sum\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtemplate_mean\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\ottawa\\lib\\site-packages\\scipy\\signal\\signaltools.py\u001b[0m in \u001b[0;36mfftconvolve\u001b[1;34m(in1, in2, mode, axes)\u001b[0m\n\u001b[0;32m    413\u001b[0m             \u001b[0msp1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrfftn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m             \u001b[0msp2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrfftn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mirfftn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msp1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msp2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfslice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_rfft_mt_safe\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\ottawa\\lib\\site-packages\\mkl_fft\\_numpy_fft.py\u001b[0m in \u001b[0;36mirfftn\u001b[1;34m(a, s, axes, norm)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \"\"\"\n\u001b[0;32m   1179\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_float_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__downcast_float128_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmkl_fft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mirfftn_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_unitary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_tot_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "same = 0\n",
    "diff = 0\n",
    "for pair in test_pairs_same:\n",
    "    i, j, _ = pair\n",
    "    left, right = test_data[i][0], test_data[j][0]\n",
    "    score = compute_similarity(left, right)\n",
    "    if (score > threshold):\n",
    "        same += 1\n",
    "for pair in test_pairs_diff:\n",
    "    i, j, _ = pair\n",
    "    left, right = test_data[i][0], test_data[j][0]\n",
    "    score = compute_similarity(left, right)\n",
    "    if (score < threshold):\n",
    "        diff += 1\n",
    "        \n",
    "same_acc = np.round(same / len(test_pairs_same), 4)\n",
    "diff_acc = np.round(diff / len(test_pairs_diff), 4)\n",
    "overall_acc = np.round((same + diff) / (len(test_pairs_same) + len(test_pairs_diff)), 4)\n",
    "np.save('test_threshold', [same_acc, diff_acc, overall_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_acc, diff_acc, overall_acc = np.load('test_threshold.npy')\n",
    "print(same_acc, diff_acc, overall_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobel_mlp_path = 'mlp/sobel_concat_2'\n",
    "\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "json_file = open(sobel_mlp_path + '.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "sobel_mlp = model_from_json(loaded_model_json)\n",
    "plot_model(sobel_mlp, to_file='sobel_mlp.png', show_shapes=True, show_layer_names=False)\n",
    "\n",
    "sobel_mlp.load_weights(sobel_mlp_path + '.h5')\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=1e-3,  decay=1e-2, amsgrad=True)\n",
    "sobel_mlp.compile(optimizer=adam,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_train_val_data = []\n",
    "edges_test_data = []\n",
    "\n",
    "for item in train_val_data:\n",
    "    img, _, filename = item\n",
    "    edges = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    edges = cv2.equalizeHist(edges)\n",
    "    sobel_hor = cv2.Sobel(edges, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    sobel_ver = cv2.Sobel(edges, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    edges = sobel_hor + sobel_ver\n",
    "    edges_train_val_data.append((edges, filename))\n",
    "\n",
    "for item in test_data:\n",
    "    img, _, filename = item\n",
    "    edges = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    edges = cv2.equalizeHist(edges)\n",
    "    sobel_hor = cv2.Sobel(edges, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    sobel_ver = cv2.Sobel(edges, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    edges = sobel_hor + sobel_ver\n",
    "    edges_test_data.append((edges, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator =     DataGenerator(train_pairs,     labels=[], data=edges_train_val_data, batch_size=32,  n_classes=2)\n",
    "val_generator =       DataGenerator(val_pairs,       labels=[], data=edges_train_val_data, batch_size=32,  n_classes=2)\n",
    "test_generator =      DataGenerator(test_pairs,      labels=[], data=edges_test_data,      batch_size=128, n_classes=2)\n",
    "test_generator_same = DataGenerator(test_pairs_same, labels=[], data=edges_test_data,      batch_size=128, n_classes=2)\n",
    "test_generator_diff = DataGenerator(test_pairs_diff, labels=[], data=edges_test_data,      batch_size=128, n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.Sequential()\n",
    "\n",
    "# model.add(tf.keras.layers.Flatten(input_shape=(64, 128)))\n",
    "# model.add(tf.keras.layers.Dense(units=512, activation='linear')\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "# model.add(tf.keras.layers.ReLU())\n",
    "# model.add(tf.keras.layers.Dropout(0.4))\n",
    "\n",
    "# model.add(tf.keras.layers.Dense(256, activation='linear')\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "# model.add(tf.keras.layers.ReLU())\n",
    "# model.add(tf.keras.layers.Dropout(0.4))\n",
    "\n",
    "# model.add(tf.keras.layers.Dense(128, activation='linear')\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "# model.add(tf.keras.layers.ReLU())\n",
    "# model.add(tf.keras.layers.Dropout(0.4))\n",
    "\n",
    "# model.add(tf.keras.layers.Dense(32, activation='linear')\n",
    "# model.add(tf.keras.layers.BatchNormalization())\n",
    "# model.add(tf.keras.layers.ReLU())\n",
    "# model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "# model.add(tf.keras.layers.Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.311970848460943 - val accuracy: 0.941120445728302\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = sobel_mlp.evaluate_generator(generator=val_generator, verbose=0, max_queue_size=5)\n",
    "print(\"val loss: {0} - val accuracy: {1}\".format(val_loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_loss, same_acc = sobel_mlp.evaluate_generator(generator=test_generator_same, verbose=0, max_queue_size=5)\n",
    "diff_loss, diff_acc = sobel_mlp.evaluate_generator(generator=test_generator_diff, verbose=0, max_queue_size=5)\n",
    "sobel_mlp_results = [same_loss, same_acc, diff_loss, diff_acc]\n",
    "np.save('sobel_mlp_results', sobel_mlp_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same loss: 0.6883313809380387 - same accuracy: 0.802675187587738\n",
      "different loss: 0.11346024669354826 - different accuracy: 0.9540323615074158\n"
     ]
    }
   ],
   "source": [
    "same_loss, same_acc, diff_loss, diff_acc = np.load('sobel_mlp_results.npy')\n",
    "print(\"same loss: {0} - same accuracy: {1}\\ndifferent loss: {2} - different accuracy: {3}\".format(same_loss, same_acc, diff_loss, diff_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
